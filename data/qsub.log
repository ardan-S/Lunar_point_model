Starting M3 client...
2024-07-04 21:33:19,604 - distributed.scheduler - WARNING - Worker failed to heartbeat for 375s; attempting restart: <WorkerState 'tcp://127.0.0.1:43529', name: 3, status: running, memory: 0, processing: 1>
2024-07-04 21:33:23,605 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-07-04 21:33:23,623 - distributed.nanny - WARNING - Restarting worker
!!!!!! FIX NAN HANDLING FOR M3 DATA !!!!!!
Processing chunk 1...
Files saved into respective CSVs
Processing chunk 2...
Files saved into respective CSVs
Processing chunk 3...
2024-07-04 21:53:19,605 - distributed.scheduler - WARNING - Worker failed to heartbeat for 496s; attempting restart: <WorkerState 'tcp://127.0.0.1:46339', name: 2, status: running, memory: 0, processing: 1>
2024-07-04 21:53:23,607 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-07-04 21:53:23,628 - distributed.nanny - WARNING - Restarting worker
Files saved into respective CSVs
Processing chunk 4...
Files saved into respective CSVs
Processing chunk 5...
Files saved into respective CSVs
Processing chunk 6...
2024-07-04 22:01:56,047 - distributed.worker - WARNING - Compute Failed
Key:       process_single_url-e1db4e9c8ce34e8b205a1b0d8840140e
State:     executing
Function:  process_single_url
args:      (('https://planetarydata.jpl.nasa.gov/img/data/m3/CH1M3_0004/DATA/20081118_20090214/200811/L2/M3G20081124T023528_V01_L2.LBL', 'https://planetarydata.jpl.nasa.gov/img/data/m3/CH1M3_0004/DATA/20081118_20090214/200811/L2/M3G20081124T023528_V01_RFL.IMG', './M3/M3_CSVs', 'M3'))
kwargs:    {}
Exception: 'TimeoutError()'

2024-07-04 22:03:19,604 - distributed.scheduler - WARNING - Worker failed to heartbeat for 439s; attempting restart: <WorkerState 'tcp://127.0.0.1:39485', name: 0, status: running, memory: 0, processing: 4>
2024-07-04 22:03:19,604 - distributed.scheduler - WARNING - Worker failed to heartbeat for 439s; attempting restart: <WorkerState 'tcp://127.0.0.1:40331', name: 3, status: running, memory: 0, processing: 4>
2024-07-04 22:03:23,606 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-07-04 22:03:23,607 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-07-04 22:03:23,623 - distributed.nanny - WARNING - Restarting worker
Files saved into respective CSVs
2024-07-04 22:03:23,626 - distributed.nanny - WARNING - Restarting worker
2024-07-04 22:08:19,604 - distributed.scheduler - WARNING - Worker failed to heartbeat for 383s; attempting restart: <WorkerState 'tcp://127.0.0.1:45125', name: 1, status: running, memory: 1, processing: 3>
2024-07-04 22:08:23,605 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-07-04 22:08:23,620 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:45125' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'process_single_url-fedaf7b0a60a1aa5171f620b829f27f7'} (stimulus_id='handle-worker-cleanup-1720127303.620121')
2024-07-04 22:08:23,623 - distributed.nanny - WARNING - Restarting worker
=>> PBS: job killed: walltime 3606 exceeded limit 3600
/sw-eb/software/Python/3.11.5-GCCcore-13.2.0/lib/python3.11/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '

====================================
CPU Time used: 00:03:39
CPU Percent: 6%
Memory usage: 11097596kb
Approx Power usage: 0.0006000000000000001
Walltime usage: 01:00:17

====================================
