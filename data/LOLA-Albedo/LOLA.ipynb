{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: File size is not a multiple of the record length. Check the file and format.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import struct\n",
    "from io import BytesIO\n",
    "\n",
    "def read_dat_file_from_url(url, columns_info):\n",
    "    \"\"\"\n",
    "    Reads a .dat file from the given URL based on the provided columns information and returns a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the .dat file.\n",
    "        columns_info (list): List of dictionaries containing column information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the data from the .dat file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = BytesIO(response.content)\n",
    "\n",
    "        # Read binary data based on columns information\n",
    "        records = []\n",
    "        row_format = ''.join([f\"{column['BYTES']}s\" for column in columns_info])\n",
    "        record_bytes = sum([column['BYTES'] for column in columns_info])\n",
    "\n",
    "        # Validate record length\n",
    "        if len(data.getvalue()) % record_bytes != 0:\n",
    "            raise ValueError(\"File size is not a multiple of the record length. Check the file and format.\")\n",
    "\n",
    "        data.seek(0)\n",
    "        while True:\n",
    "            record = data.read(record_bytes)\n",
    "            if not record:\n",
    "                break\n",
    "            if len(record) != record_bytes:\n",
    "                raise ValueError(f\"Record size mismatch. Expected {record_bytes}, got {len(record)}.\")\n",
    "            unpacked_record = struct.unpack(row_format, record)\n",
    "            parsed_record = []\n",
    "            for i, value in enumerate(unpacked_record):\n",
    "                column = columns_info[i]\n",
    "                if column['DATA_TYPE'] == 'LSB_INTEGER':\n",
    "                    parsed_value = int.from_bytes(value, byteorder='little', signed=True)\n",
    "                elif column['DATA_TYPE'] == 'LSB_UNSIGNED_INTEGER':\n",
    "                    parsed_value = int.from_bytes(value, byteorder='little', signed=False)\n",
    "                else:\n",
    "                    parsed_value = value.decode('utf-8').strip()\n",
    "                if parsed_value == column.get('MISSING_CONSTANT'):\n",
    "                    parsed_value = None\n",
    "                parsed_record.append(parsed_value)\n",
    "            records.append(parsed_record)\n",
    "\n",
    "        df = pd.DataFrame(records, columns=[col['NAME'] for col in columns_info])\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the .dat file: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_fmt_file(fmt_content):\n",
    "    \"\"\"\n",
    "    Parses the content of LOLARDR.FMT file to extract column definitions.\n",
    "\n",
    "    Parameters:\n",
    "        fmt_content (str): Content of the LOLARDR.FMT file as a string.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries with column definitions.\n",
    "    \"\"\"\n",
    "    lines = fmt_content.splitlines()\n",
    "    columns = []\n",
    "    column = {}\n",
    "    for line in lines:\n",
    "        if 'OBJECT = COLUMN' in line:\n",
    "            column = {}\n",
    "        elif 'COLUMN_NUMBER' in line:\n",
    "            column['COLUMN_NUMBER'] = int(line.split('=')[1].strip()) if '=' in line else None\n",
    "        elif 'BYTES' in line:\n",
    "            column['BYTES'] = int(line.split('=')[1].strip()) if '=' in line else None\n",
    "        elif 'START_BYTE' in line:\n",
    "            column['START_BYTE'] = int(line.split('=')[1].strip()) if '=' in line else None\n",
    "        elif 'NAME' in line:\n",
    "            column['NAME'] = line.split('=')[1].strip().strip('\"') if '=' in line else None\n",
    "        elif 'DATA_TYPE' in line:\n",
    "            column['DATA_TYPE'] = line.split('=')[1].strip() if '=' in line else None\n",
    "        elif 'UNIT' in line:\n",
    "            column['UNIT'] = line.split('=')[1].strip().strip(\"'\") if '=' in line else None\n",
    "        elif 'MISSING_CONSTANT' in line:\n",
    "            column['MISSING_CONSTANT'] = int(line.split('=')[1].strip()) if '=' in line else None\n",
    "        elif 'END_OBJECT' in line:\n",
    "            columns.append(column)\n",
    "    return columns\n",
    "\n",
    "# Example usage\n",
    "url = 'https://pds-geosciences.wustl.edu/lro/lro-l-lola-3-rdr-v1/lrolol_1xxx/data/lola_rdr/lro_es_03/lolardr_123201617.dat'\n",
    "fmt_url = 'https://pds-geosciences.wustl.edu/lro/lro-l-lola-2-edr-v1/lrolol_0xxx/label/lolardr.fmt'\n",
    "\n",
    "# Fetch and parse the .fmt file\n",
    "fmt_response = requests.get(fmt_url)\n",
    "fmt_response.raise_for_status()\n",
    "fmt_content = fmt_response.text\n",
    "columns_info = parse_fmt_file(fmt_content)\n",
    "\n",
    "# Read the .dat file based on the parsed .fmt information\n",
    "df = read_dat_file_from_url(url, columns_info)\n",
    "if df is not None:\n",
    "    print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
