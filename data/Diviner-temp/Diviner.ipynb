{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glymur\n",
    "import matplotlib.pyplot as plt\n",
    "import julian\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def parse_metadata(file_path):\n",
    "    metadata = {}\n",
    "    enc = 'utf-8'\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=enc) as file:\n",
    "            for line in file:\n",
    "                if \"=\" in line:\n",
    "                    key, value = line.split(\"=\", 1)\n",
    "                    key = key.strip()\n",
    "                    value = value.strip().strip('\"')\n",
    "                    metadata[key] = value\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error decoding the file {file_path} with encoding {enc}\")\n",
    "    return metadata\n",
    "\n",
    "def extract_jp2_data(jp2_file_path):\n",
    "    jp2 = glymur.Jp2k(jp2_file_path)\n",
    "    image_data = jp2[:]\n",
    "    return image_data\n",
    "\n",
    "def convert_dn_to_val(dn, scaling_factor, offset, missing_constant):\n",
    "    val = np.full(dn.shape, np.nan)\n",
    "    mask = (dn != missing_constant)\n",
    "    val[mask] = (dn[mask] * scaling_factor) + offset\n",
    "    return val\n",
    "\n",
    "def clean_metadata_value(value):\n",
    "    try:\n",
    "        cleaned_value = ''.join(filter(lambda x: x.isdigit() or x in ['.', '-'], value))\n",
    "        return float(cleaned_value)\n",
    "    except ValueError:\n",
    "        print(f\"Error converting metadata value to float: {value}\")\n",
    "        return None\n",
    "\n",
    "def generate_coordinates(image_shape, metadata):\n",
    "    lines, samples = image_shape\n",
    "    line_projection_offset = clean_metadata_value(metadata['LINE_PROJECTION_OFFSET'])\n",
    "    sample_projection_offset = clean_metadata_value(metadata['SAMPLE_PROJECTION_OFFSET'])\n",
    "    map_scale = clean_metadata_value(metadata['MAP_SCALE'])\n",
    "    center_lat = clean_metadata_value(metadata['CENTER_LATITUDE'])\n",
    "    center_lon = clean_metadata_value(metadata['CENTER_LONGITUDE'])\n",
    "    map_resolution = clean_metadata_value(metadata['MAP_RESOLUTION'])\n",
    "\n",
    "    # Assuming MAP_RESOLUTION is in degrees per pixel\n",
    "    deg_per_pixel = 1.0 / map_resolution\n",
    "\n",
    "    lats = center_lat - ((np.arange(lines) - line_projection_offset) * deg_per_pixel)\n",
    "    lons = center_lon + ((np.arange(samples) - sample_projection_offset) * deg_per_pixel)\n",
    "\n",
    "    # Ensure lons and lats fall within specified min/max values\n",
    "    lats = np.clip(lats, clean_metadata_value(metadata['MINIMUM_LATITUDE']), clean_metadata_value(metadata['MAXIMUM_LATITUDE']))\n",
    "    lons = np.mod(lons, 360.0)\n",
    "\n",
    "    return np.meshgrid(lons, lats)\n",
    "\n",
    "def save_to_csv(lons, lats, julian_dates, output_csv_path):\n",
    "    data = {\n",
    "        'Longitude': lons.flatten(),\n",
    "        'Latitude': lats.flatten(),\n",
    "        'Julian Date': julian_dates.flatten(),\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "def process_image(metadata, jp2_file_path, output_csv_path, data_type, save_data=True):\n",
    "\n",
    "    accepted_data_types = ['date', 'temp']\n",
    "    if data_type not in accepted_data_types:\n",
    "        raise ValueError(f\"Invalid data type '{data_type}'. Accepted values are: {accepted_data_types}\")\n",
    "    \n",
    "    image_data = extract_jp2_data(jp2_file_path)\n",
    "\n",
    "    scaling_factor = clean_metadata_value(metadata.get('SCALING_FACTOR', 1))  # Default to 1 if not found\n",
    "    offset = clean_metadata_value(metadata.get('OFFSET', 0))  # Default to 0 if not found\n",
    "    missing_constant = (metadata.get('IMAGE_MISSING_CONSTANT', -32768))\n",
    "\n",
    "    dn_value = image_data[0, 0]\n",
    "\n",
    "    lons, lats = generate_coordinates(image_data.shape, metadata)\n",
    "\n",
    "    if data_type == 'date':\n",
    "\n",
    "        output_vals = convert_dn_to_val(image_data, scaling_factor, offset, missing_constant)\n",
    "        julian_date = convert_dn_to_val(np.array([dn_value]), scaling_factor, offset, missing_constant)[0]\n",
    "\n",
    "        if ~np.isnan(julian_date):\n",
    "            calendar_date = julian.from_jd(julian_date, fmt='jd')\n",
    "            print(f\"Digital Number (DN): {dn_value} -> Julian Date: {julian_date} -> Gregorian Date: {calendar_date}\\n\")\n",
    "\n",
    "    elif data_type == 'temp':\n",
    "        output_vals = convert_dn_to_val(image_data, scaling_factor, offset, missing_constant)\n",
    "\n",
    "    if save_data:\n",
    "        save_to_csv(lons, lats, output_vals, output_csv_path)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Longitude': lons.flatten(),\n",
    "        'Latitude': lats.flatten(),\n",
    "        data_type: output_vals.flatten(),\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Optimize the data types of the dataframes\n",
    "    def optimize_df(df):\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nans in Julian Dates: 1870572 out of 14531344 (12.87%)\n",
      "Number of Nans in Temperatures: 2550840 out of 14531344 (17.55%)\n",
      "\n",
      "    Longitude  Latitude  date  temp\n",
      "0  344.918518      90.0   NaN   NaN\n",
      "1  344.926422      90.0   NaN   NaN\n",
      "2  344.934357      90.0   NaN   NaN\n",
      "3  344.942261      90.0   NaN   NaN\n",
      "4  344.950165      90.0   NaN   NaN \n",
      "\n",
      "Min/max longitude: 0.00395735539495945, 359.99603271484375\n",
      "Min/max latitude: 75.0, 90.0\n",
      "Min/max Gregorian date: 2016-01-12 18:00:00, 2016-02-10 00:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_metadata_file_path = '/home/as5023/ACSE/irp-as5023/data/Diviner-temp/dgdr_jd_avg_poln_20160111d_240_jp2.lbl'\n",
    "date_metadata = parse_metadata(date_metadata_file_path)\n",
    "date_jp2_file_path = date_metadata_file_path.replace('.lbl', '.jp2')\n",
    "\n",
    "temp_metadata_file_path = '/home/as5023/ACSE/irp-as5023/data/Diviner-temp/dgdr_tbol_avg_poln_20160111d_240_jp2.lbl'\n",
    "temp_metadata = parse_metadata(temp_metadata_file_path)\n",
    "temp_jp2_file_path = temp_metadata_file_path.replace('.lbl', '.jp2')\n",
    "\n",
    "output_csv_path = 'diviner_data.csv'\n",
    "\n",
    "if os.path.exists(date_jp2_file_path) and os.path.exists(temp_jp2_file_path):\n",
    "    date_df = process_image(date_metadata, date_jp2_file_path, output_csv_path, 'date', False)\n",
    "    temp_df = process_image(temp_metadata, temp_jp2_file_path, output_csv_path, 'temp', False)\n",
    "\n",
    "    date_df = optimize_df(date_df)\n",
    "    temp_df = optimize_df(temp_df)\n",
    "    date_df['date'] = pd.to_numeric(date_df['date'], errors='coerce')\n",
    "    temp_df['temp'] = pd.to_numeric(temp_df['temp'], errors='coerce')\n",
    "    print(f\"Number of Nans in Julian Dates: {np.isnan(date_df[\"date\"]).sum()} out of {np.prod(date_df[\"date\"].shape)} ({(np.isnan(date_df[\"date\"]).sum()/np.prod(date_df[\"date\"].shape)*100):.2f}%)\")\n",
    "    print(f\"Number of Nans in Temperatures: {np.isnan(temp_df[\"temp\"]).sum()} out of {np.prod(temp_df[\"temp\"].shape)} ({(np.isnan(temp_df[\"temp\"]).sum()/np.prod(temp_df[\"temp\"].shape)*100):.2f}%)\\n\")\n",
    "\n",
    "\n",
    "    # Check if coordinates align\n",
    "    if not (date_df[['Longitude', 'Latitude']].equals(temp_df[['Longitude', 'Latitude']])):\n",
    "        raise ValueError(\"The coordinates in date_df and temp_df do not align\")\n",
    "    \n",
    "    # #If they align, drop the duplicate columns and concat\n",
    "    temp_df = temp_df.drop(columns=['Longitude', 'Latitude'])\n",
    "    combined_df = pd.concat([date_df, temp_df], axis=1)\n",
    "\n",
    "    # # # Print the combined dataframe's head and other information\n",
    "    print(combined_df.head(), '\\n')\n",
    "    print(f'Min/max longitude: {combined_df[\"Longitude\"].min()}, {combined_df[\"Longitude\"].max()}')\n",
    "    print(f'Min/max latitude: {combined_df[\"Latitude\"].min()}, {combined_df[\"Latitude\"].max()}')\n",
    "\n",
    "    min_jd = np.nanmin(combined_df[\"date\"])\n",
    "    max_jd = np.nanmax(combined_df[\"date\"])\n",
    "\n",
    "\n",
    "    if not np.isnan(min_jd) and not np.isnan(max_jd):\n",
    "        print(f'Min/max Gregorian date: {julian.from_jd(min_jd, fmt=\"jd\")}, {julian.from_jd(max_jd, fmt=\"jd\")}\\n')\n",
    "\n",
    "else:\n",
    "    print(f\"JP2 file not found at {jp2_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
