/var/spool/pbs/mom_priv/jobs/21036.pbs-7.SC: line 27: ./logs/update_venv.log: No such file or directory
Percentage of each label in the dataset:
Label 0: 60.92%
Label 1: 6.74%
Label 2: 27.00%
Label 3: 3.16%
Label 4: 1.98%
Label 5: 0.15%
Label 6: 0.04%
Label 7: 0.00%
Percentage of each label after resampling:
Label 0: 25.00%
Label 1: 12.50%
Label 2: 12.50%
Label 3: 12.50%
Label 4: 12.50%
Label 5: 12.50%
Label 6: 12.50%
Label 7: 1.90%
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.1, beta: 0.05, weight decay: 0.0005
New best hyperparameters: {'batch_size': 131072, 'learning_rate': 0.0001, 'dropout_rate': 0.1, 'beta': 0.05, 'weight_decay': 0.0005}
New best loss: 0.7888
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.1, beta: 0.05, weight decay: 0.0001
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.1, beta: 0.05, weight decay: 5e-05
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.1, beta: 0.1, weight decay: 0.0005
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.1, beta: 0.1, weight decay: 0.0001
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.1, beta: 0.1, weight decay: 5e-05
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.1, beta: 0.15, weight decay: 0.0005
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.1, beta: 0.15, weight decay: 0.0001
New best hyperparameters: {'batch_size': 131072, 'learning_rate': 0.0001, 'dropout_rate': 0.1, 'beta': 0.15, 'weight_decay': 0.0001}
New best loss: 0.7569
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.1, beta: 0.15, weight decay: 5e-05
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.15, beta: 0.05, weight decay: 0.0005
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.15, beta: 0.05, weight decay: 0.0001
New best hyperparameters: {'batch_size': 131072, 'learning_rate': 0.0001, 'dropout_rate': 0.15, 'beta': 0.05, 'weight_decay': 0.0001}
New best loss: 0.7391
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.15, beta: 0.05, weight decay: 5e-05
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.15, beta: 0.1, weight decay: 0.0005
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.15, beta: 0.1, weight decay: 0.0001
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.15, beta: 0.1, weight decay: 5e-05
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.15, beta: 0.15, weight decay: 0.0005
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.15, beta: 0.15, weight decay: 0.0001
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.15, beta: 0.15, weight decay: 5e-05
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.2, beta: 0.05, weight decay: 0.0005
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.2, beta: 0.05, weight decay: 0.0001
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.2, beta: 0.05, weight decay: 5e-05
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.2, beta: 0.1, weight decay: 0.0005
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.2, beta: 0.1, weight decay: 0.0001
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.2, beta: 0.1, weight decay: 5e-05
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.2, beta: 0.15, weight decay: 0.0005
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.2, beta: 0.15, weight decay: 0.0001
Training with batch size: 131072, learning rate: 0.0001, dropout rate: 0.2, beta: 0.15, weight decay: 5e-05
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.1, beta: 0.05, weight decay: 0.0005
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.1, beta: 0.05, weight decay: 0.0001
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.1, beta: 0.05, weight decay: 5e-05
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.1, beta: 0.1, weight decay: 0.0005
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.1, beta: 0.1, weight decay: 0.0001
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.1, beta: 0.1, weight decay: 5e-05
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.1, beta: 0.15, weight decay: 0.0005
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.1, beta: 0.15, weight decay: 0.0001
New best hyperparameters: {'batch_size': 131072, 'learning_rate': 5e-05, 'dropout_rate': 0.1, 'beta': 0.15, 'weight_decay': 0.0001}
New best loss: 0.7156
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.1, beta: 0.15, weight decay: 5e-05
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.15, beta: 0.05, weight decay: 0.0005
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.15, beta: 0.05, weight decay: 0.0001
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.15, beta: 0.05, weight decay: 5e-05
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.15, beta: 0.1, weight decay: 0.0005
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.15, beta: 0.1, weight decay: 0.0001
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.15, beta: 0.1, weight decay: 5e-05
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.15, beta: 0.15, weight decay: 0.0005
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.15, beta: 0.15, weight decay: 0.0001
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.15, beta: 0.15, weight decay: 5e-05
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.2, beta: 0.05, weight decay: 0.0005
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.2, beta: 0.05, weight decay: 0.0001
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.2, beta: 0.05, weight decay: 5e-05
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.2, beta: 0.1, weight decay: 0.0005
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.2, beta: 0.1, weight decay: 0.0001
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.2, beta: 0.1, weight decay: 5e-05
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.2, beta: 0.15, weight decay: 0.0005
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.2, beta: 0.15, weight decay: 0.0001
Training with batch size: 131072, learning rate: 5e-05, dropout rate: 0.2, beta: 0.15, weight decay: 5e-05
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.1, beta: 0.05, weight decay: 0.0005
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.1, beta: 0.05, weight decay: 0.0001
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.1, beta: 0.05, weight decay: 5e-05
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.1, beta: 0.1, weight decay: 0.0005
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.1, beta: 0.1, weight decay: 0.0001
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.1, beta: 0.1, weight decay: 5e-05
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.1, beta: 0.15, weight decay: 0.0005
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.1, beta: 0.15, weight decay: 0.0001
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.1, beta: 0.15, weight decay: 5e-05
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.15, beta: 0.05, weight decay: 0.0005
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.15, beta: 0.05, weight decay: 0.0001
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.15, beta: 0.05, weight decay: 5e-05
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.15, beta: 0.1, weight decay: 0.0005
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.15, beta: 0.1, weight decay: 0.0001
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.15, beta: 0.1, weight decay: 5e-05
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.15, beta: 0.15, weight decay: 0.0005
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.15, beta: 0.15, weight decay: 0.0001
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.15, beta: 0.15, weight decay: 5e-05
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.2, beta: 0.05, weight decay: 0.0005
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.2, beta: 0.05, weight decay: 0.0001
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.2, beta: 0.05, weight decay: 5e-05
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.2, beta: 0.1, weight decay: 0.0005
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.2, beta: 0.1, weight decay: 0.0001
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.2, beta: 0.1, weight decay: 5e-05
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.2, beta: 0.15, weight decay: 0.0005
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.2, beta: 0.15, weight decay: 0.0001
Training with batch size: 131072, learning rate: 1e-05, dropout rate: 0.2, beta: 0.15, weight decay: 5e-05

Best hyperparameters:
Batch size: 131072 out of [131072]
Learning rate: 5e-05 out of [0.0001, 5e-05, 1e-05]
Dropout rate: 0.1 out of [0.1, 0.15, 0.2]
Beta: 0.15 out of [0.05, 0.1, 0.15]
Weight decay: 0.0001 out of [0.0005, 0.0001, 5e-05]

Best loss: 0.7156
Best MSE: 1.3013
R² for best hyperparams: 0.7261
Completed on Wed 28 Aug 16:55:40 BST 2024

====================================
CPU Time used: 29:00:17
CPU Percent: 151%
Memory usage: 23726972kb
Approx Power usage: 0.348
Walltime usage: 19:11:29

====================================
